{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Notebook 1: preprocessing_and_splits.ipynb\n",
        "# FIXED: correct dataset paths + remove duplicate images in part1/part2\n",
        "\n",
        "import os, sys, math\n",
        "import numpy as np, pandas as pd\n",
        "import cv2\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"TF:\", tf.__version__)\n",
        "print(\"GPU:\", tf.config.list_physical_devices('GPU'))\n",
        "for g in tf.config.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(g, True)\n",
        "\n"
      ],
      "metadata": {
        "id": "0ERFpWLDjfkE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d949648-475e-40ce-bb77-58c27c6c2024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF: 2.19.0\n",
            "GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# 1) Correct dataset paths\n",
        "# ======================================================\n",
        "ZIP_PATH = \"/content/drive/MyDrive/HAMDATASET.zip\"\n",
        "ROOT = \"/content/HAMDATASET\"\n",
        "IMAGE_FOLDER = \"/content/drive/MyDrive/HAM10000_images\"\n",
        "META_CSV = f\"/content/HAM10000_metadata.csv\"\n",
        "\n",
        "# Extract ZIP (if not already extracted)\n",
        "if not os.path.exists(ROOT):\n",
        "    !unzip -q \"/content/drive/MyDrive/HAMDATASET.zip\" -d /content/\n",
        "\n",
        "# Create folder for unique images\n",
        "os.makedirs(IMAGE_FOLDER, exist_ok=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MCGVIiP9jot5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec82141a-f3cd-4ae8-986c-0f2b0d18b753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/HAMDATASET.zip, /content/drive/MyDrive/HAMDATASET.zip.zip or /content/drive/MyDrive/HAMDATASET.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# 2) Merge part1 + part2 WITHOUT duplicates\n",
        "# ======================================================\n",
        "part1 = f\"/content/HAM10000_images_part_1\"\n",
        "part2 = f\"/content/HAM10000_images_part_2\"\n",
        "\n",
        "all_files = set()          # track unique filenames\n",
        "duplicate_count = 0\n",
        "moved = 0\n",
        "\n",
        "for folder in [part1, part2]:\n",
        "    for fname in os.listdir(folder):\n",
        "        if not fname.endswith(\".jpg\"):\n",
        "            continue\n",
        "\n",
        "        src = os.path.join(folder, fname)\n",
        "        dst = os.path.join(IMAGE_FOLDER, fname)\n",
        "\n",
        "        if fname in all_files:\n",
        "            duplicate_count += 1\n",
        "            continue   # skip duplicates\n",
        "\n",
        "        all_files.add(fname)\n",
        "        os.system(f\"cp '{src}' '{dst}'\")\n",
        "        moved += 1\n",
        "\n",
        "print(f\"Unique images moved: {moved}\")\n",
        "print(f\"Duplicates skipped: {duplicate_count}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oe8EDOMrjr96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_FOLDER = \"/content/drive/MyDrive/HAM10000_images\""
      ],
      "metadata": {
        "id": "oe5r9SDGltnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# 3) Load metadata and attach correct filepath\n",
        "# ======================================================\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/HAM10000_metadata.csv\")\n",
        "print(\"Metadata loaded:\", df.shape)\n",
        "\n",
        "df['filename'] = df['image_id'] + \".jpg\"\n",
        "df['filepath'] = df['filename'].apply(lambda x: os.path.join(IMAGE_FOLDER, x))\n",
        "\n",
        "# Keep only rows where the image exists (no duplicates / missing)\n",
        "df = df[df['filepath'].apply(os.path.exists)].reset_index(drop=True)\n",
        "print(\"After matching images:\", df.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "BBkJUToajvIA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b589e327-1733-44fd-dee2-caf90ac05ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata loaded: (10015, 7)\n",
            "After matching images: (8896, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# 4) Label encoding (7 classes)\n",
        "# ======================================================\n",
        "le = LabelEncoder()\n",
        "df['dx'] = df['dx'].astype(str)\n",
        "df['label'] = le.fit_transform(df['dx'])\n",
        "print(\"Label map:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
        "\n"
      ],
      "metadata": {
        "id": "xbeDEDf9jx7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42bebc9-67d8-4150-a586-b2e9a6ff700f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label map: {'akiec': np.int64(0), 'bcc': np.int64(1), 'bkl': np.int64(2), 'df': np.int64(3), 'mel': np.int64(4), 'nv': np.int64(5), 'vasc': np.int64(6)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# 5) Lesion-level split (NO leakage)\n",
        "# ======================================================\n",
        "group_col = 'lesion_id' if 'lesion_id' in df.columns else 'image_id'\n",
        "\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
        "train_idx, test_idx = next(gss.split(df, groups=df[group_col]))\n",
        "\n",
        "df_trainval = df.iloc[train_idx].reset_index(drop=True)\n",
        "df_test = df.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.12, random_state=42)\n",
        "tr_idx, val_idx = next(gss2.split(df_trainval, groups=df_trainval[group_col]))\n",
        "\n",
        "df_train = df_trainval.iloc[tr_idx].reset_index(drop=True)\n",
        "df_val   = df_trainval.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "print(\"Train size:\", len(df_train))\n",
        "print(\"Val size:\", len(df_val))\n",
        "print(\"Test size:\", len(df_test))\n",
        "\n"
      ],
      "metadata": {
        "id": "COOJfAnOjzix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36079c91-adc7-47e8-abf2-ffb9f22158a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 6255\n",
            "Val size: 861\n",
            "Test size: 1780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# 6) Oversampling TRAIN ONLY (balanced)\n",
        "# ======================================================\n",
        "target = df_train['label'].value_counts().max()\n",
        "\n",
        "rows = []\n",
        "for lbl, g in df_train.groupby('label'):\n",
        "    if len(g) < target:\n",
        "        extra = g.sample(target - len(g), replace=True, random_state=42)\n",
        "        rows.append(pd.concat([g, extra], axis=0))\n",
        "    else:\n",
        "        rows.append(g)\n",
        "\n",
        "df_train_bal = pd.concat(rows).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(\"Balanced train counts:\\n\", df_train_bal['dx'].value_counts())\n",
        "\n"
      ],
      "metadata": {
        "id": "MYFeFfCOj1K5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c193e8e3-3b26-4106-a8dc-ce8b8f042ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced train counts:\n",
            " dx\n",
            "bcc      4213\n",
            "bkl      4213\n",
            "df       4213\n",
            "vasc     4213\n",
            "nv       4213\n",
            "mel      4213\n",
            "akiec    4213\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# 7) Save splits for other notebooks\n",
        "# ======================================================\n",
        "os.makedirs(\"/content/drive/MyDrive/splits\", exist_ok=True)\n",
        "df_train_bal.to_csv(\"/content/drive/MyDrive/splits/df_train.csv\", index=False)\n",
        "df_val.to_csv(\"/content/drive/MyDrive/splits/df_val.csv\", index=False)\n",
        "df_test.to_csv(\"/content/drive/MyDrive/splits/df_test.csv\", index=False)\n",
        "\n",
        "print(\"Saved splits to /content/drive/MyDrive/splits/\")"
      ],
      "metadata": {
        "id": "AT3PpB2Tj2ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45f7863c-a1fa-422c-a283-95937ec6753e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved splits to /content/drive/MyDrive/splits/\n"
          ]
        }
      ]
    }
  ]
}